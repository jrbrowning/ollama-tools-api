services:
  traditional_model:
    build:
      context: .
      dockerfile: lib/Dockerfile.template
    container_name: traditional_model
    expose:
      - "11434" # Exposes to other containers on the same Docker network
    ports:
      - "11435:11434" #container listens on 11434, and if you curl localhost:11435 from your host machine, it reaches that container
    environment:
      MODEL_NAME: ${TRADITIONAL_MODEL}
      OFFLINE_MODE: "false"
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - internal_net
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"

  traditional_model_alt:
    build:
      context: .
      dockerfile: lib/Dockerfile.template
    container_name: traditional_model_alt
    expose:
      - "11434"
    ports:
      - "11437:11434"
    environment:
      MODEL_NAME: ${TRADITIONAL_MODEL_ALT}
      OFFLINE_MODE: "false"
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - internal_net
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"

  reasoning_model:
    build:
      context: .
      dockerfile: lib/Dockerfile.template
    container_name: reasoning_model
    expose:
      - "11434"
    ports:
      - "11436:11434"
    environment:
      MODEL_NAME: ${REASONING_MODEL}
      OFFLINE_MODE: "false"
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - internal_net
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"

  reasoning_model_alt:
    build:
      context: .
      dockerfile: lib/Dockerfile.template
    container_name: reasoning_model_alt
    expose:
      - "11434"
    ports:
      - "11438:11434"
    environment:
      MODEL_NAME: ${REASONING_MODEL_ALT}
      OFFLINE_MODE: "false"
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - internal_net
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"

  ollama-webui:
    image: ghcr.io/ollama-webui/ollama-webui:main
    container_name: ollama-webui
    ports:
      - "8080:8080"
    environment:
      - OLLAMA_BASE_URL=http://host.docker.internal:11434
      - WEBUI_AUTH__ENABLE=true
      - WEBUI_AUTH__REGISTER_DEFAULT_ADMIN=true
      - WEBUI_AUTH__DEFAULT_ADMIN_EMAIL=admin@example.com
      - WEBUI_AUTH__DEFAULT_ADMIN_PASSWORD=supersecurepassword
      - WEBUI_SECRET_KEY=your-very-secure-random-secret
      - OLLAMA_WEBUI_DEFAULT_USER=overlord
      - OLLAMA_WEBUI_DEFAULT_PASSWORD=ollama-local-army-ftw
      - OLLAMA_DEFAULT_MODEL=llama3.1:8b
    restart: unless-stopped
    networks:
      - internal_net

volumes:
  ollama_data:

networks:
  internal_net:
    driver: bridge
