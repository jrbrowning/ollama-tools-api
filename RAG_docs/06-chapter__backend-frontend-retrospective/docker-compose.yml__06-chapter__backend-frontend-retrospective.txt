CONTEXT: FILE: docker-compose.yml
BRANCH: 06-chapter__backend-frontend-retrospective

services:
  traditional_model:
    build:
      context: .
      dockerfile: lib/Dockerfile.template
    container_name: traditional_model
    expose:
      - "11434" # Exposes to other containers on the same Docker network
    ports:
      - "11435:11434" #container listens on 11434, and if you curl localhost:11435 from your host machine, it reaches that container
    environment:
      MODEL_NAME: ${TRADITIONAL_MODEL}
      OFFLINE_MODE: "false"
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - internal_net
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"

  traditional_model_alt:
    build:
      context: .
      dockerfile: lib/Dockerfile.template
    container_name: traditional_model_alt
    expose:
      - "11434"
    ports:
      - "11437:11434"
    environment:
      MODEL_NAME: ${TRADITIONAL_MODEL_ALT}
      OFFLINE_MODE: "false"
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - internal_net
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"

  reasoning_model:
    build:
      context: .
      dockerfile: lib/Dockerfile.template
    container_name: reasoning_model
    expose:
      - "11434"
    ports:
      - "11436:11434"
    environment:
      MODEL_NAME: ${REASONING_MODEL}
      OFFLINE_MODE: "false"
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - internal_net
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"

  reasoning_model_alt:
    build:
      context: .
      dockerfile: lib/Dockerfile.template
    container_name: reasoning_model_alt
    expose:
      - "11434"
    ports:
      - "11438:11434"
    environment:
      MODEL_NAME: ${REASONING_MODEL_ALT}
      OFFLINE_MODE: "false"
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - internal_net
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"

  api_server:
    build:
      context: ./fastapi_server
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
      - "5678:5678" # Debuggy port for FastAPI
    volumes:
      - ./fastapi_server/api:/app/api
    container_name: api_server
    environment:
      LOG_LEVEL: "INFO"
      MODEL_SERVERS: "traditional_model,reasoning_model,traditional_model_alt,reasoning_model_alt"
      DEFAULT_MODEL_SERVER: "traditional_model"
      ENABLE_LOCAL_GPU_MODEL: ${ENABLE_LOCAL_GPU_MODEL}
      LOCAL_GPU: ${LOCAL_GPU}
      TRADITIONAL_MODEL: ${TRADITIONAL_MODEL}
      TRADITIONAL_MODEL_ALT: ${TRADITIONAL_MODEL_ALT}
      REASONING_MODEL: ${REASONING_MODEL}
      REASONING_MODEL_ALT: ${REASONING_MODEL_ALT}
    depends_on:
      - traditional_model
      - reasoning_model
      - traditional_model_alt
      - reasoning_model_alt
    networks:
      - internal_net
    extra_hosts:
      - "host.docker.internal:host-gateway"
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"

  ollama-webui:
    image: ghcr.io/ollama-webui/ollama-webui:main
    container_name: ollama-webui
    ports:
      - "8080:8080"
    environment:
      - WEBUI_AUTH__ENABLE=true
      - WEBUI_AUTH__REGISTER_DEFAULT_ADMIN=true
      - WEBUI_AUTH__DEFAULT_ADMIN_EMAIL=admin@example.com
      - WEBUI_AUTH__DEFAULT_ADMIN_PASSWORD=supersecurepassword
      - WEBUI_AUTH__ENABLE=false
      - WEBUI_SECRET_KEY=your-very-secure-random-secret
      - OLLAMA_WEBUI_DEFAULT_USER=overlord
      - OLLAMA_WEBUI_DEFAULT_PASSWORD=ollama-local-army-ftw
      - OLLAMA_DEFAULT_MODEL=llama3.1:8b
    depends_on:
      - traditional_model
      - reasoning_model
    restart: unless-stopped
    networks:
      - internal_net

volumes:
  ollama_data:

networks:
  internal_net:
    driver: bridge
