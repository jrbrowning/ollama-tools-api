CONTEXT: FILE: fastapi_server/api/routes/stream.py
BRANCH: 06-chapter__backend-frontend-retrospective

# File: api/routes/stream.py

from fastapi import APIRouter, HTTPException
from fastapi.responses import Response
from models.llm_request import LLMRequest

from api.config.model_routes import model_map, model_service_map, protocol_map
from api.dispatch.toolchain_stream import dispatch_toolchain_stream
from api.dispatch.chat_stream import dispatch_chat_stream

router = APIRouter(prefix="/stream/v1")


@router.post("/chat", response_model=None)
async def chat_stream_handler(payload: LLMRequest) -> Response:
    model_id = payload.model_container

    if model_id not in model_map:
        raise HTTPException(
            status_code=400,
            detail=f"Unknown model: {model_id}. Available models: {list(model_map.keys())}",
        )

    model_name = model_map[model_id]
    base_url = model_service_map.get(model_id)
    protocol = protocol_map.get(model_id, "openai")

    if not model_name:
        raise HTTPException(
            status_code=400,
            detail=f"Model {model_id} is not configured. Check environment variables.",
        )

    if not base_url:
        raise HTTPException(
            status_code=400,
            detail=f"No service configured for model: {model_id}",
        )

    return await dispatch_chat_stream(
        payload=payload,
        model_name=model_name,
        base_url=f"{base_url}/v1",
        protocol=protocol,
    )


@router.post("/toolchain", response_model=None)
async def toolchain_stream_handler(payload: LLMRequest) -> Response:
    model_id = payload.model_container

    if model_id not in model_map:
        raise HTTPException(
            status_code=400,
            detail=f"Unknown model: {model_id}. Available models: {list(model_map.keys())}",
        )

    model_name = model_map[model_id]
    base_url = model_service_map.get(model_id)
    protocol = protocol_map.get(model_id, "openai")

    if not model_name:
        raise HTTPException(
            status_code=400,
            detail=f"Model {model_id} is not configured. Check environment variables.",
        )

    if not base_url:
        raise HTTPException(
            status_code=400,
            detail=f"No service configured for model: {model_id}",
        )

    return await dispatch_toolchain_stream(
        payload=payload,
        model_name=model_name,
        base_url=f"{base_url}/v1",
        protocol=protocol,
    )
