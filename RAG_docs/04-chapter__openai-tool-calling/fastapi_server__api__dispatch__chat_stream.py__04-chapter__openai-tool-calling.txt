CONTEXT: FILE: fastapi_server/api/dispatch/chat_stream.py
BRANCH: 04-chapter__openai-tool-calling

# File: api/dispatch/chat_stream.py

from fastapi import HTTPException
from fastapi.responses import StreamingResponse
from models.llm_request import LLMRequest

from api.handlers.openai.chat_stream import openai_chat_completion_stream

# from api.handlers.mcp.chat_stream import mcp_chat_completion_stream


async def dispatch_chat_stream(
    payload: LLMRequest,
    model_name: str,
    base_url: str,
    protocol: str,
) -> StreamingResponse:
    if protocol == "openai":
        return await openai_chat_completion_stream(
            stage_id=payload.stage_id,
            base_url=base_url,
            model_name=model_name,
            user_prompt=payload.user_prompt,
            system_prompt=payload.system_prompt,
            max_tokens=payload.max_tokens,
            temperature=payload.temperature,
        )

    # if protocol == "mcp":
    #     return await mcp_chat_completion_stream(
    #         stage_id=payload.stage_id,
    #         base_url=base_url,
    #         model_name=model_name,
    #         user_prompt=payload.user_prompt,
    #         system_prompt=payload.system_prompt,
    #         max_tokens=payload.max_tokens,
    #         temperature=payload.temperature,
    #     )

    raise HTTPException(status_code=501, detail=f"Unsupported protocol: {protocol}")
