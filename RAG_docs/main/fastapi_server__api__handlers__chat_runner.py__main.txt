CONTEXT: FILE: fastapi_server/api/handlers/chat_runner.py
BRANCH: main

# File: api/handlers/logic_runner.py

import asyncio
from typing import List, Union

from fastapi.responses import JSONResponse, StreamingResponse
from openai import AsyncOpenAI
from openai.types.chat import (
    ChatCompletionSystemMessageParam,
    ChatCompletionUserMessageParam,
)
from models.events import serialize_sse_event, ChatCompletionStreamPayload, DonePayload, CancelPayload, ErrorPayload
from models.llm_response import TextStageOutput

async def _stream_chat_response(
    stage_id: str,
    client: AsyncOpenAI,
    messages: List[Union[ChatCompletionSystemMessageParam, ChatCompletionUserMessageParam]],
    model_name: str,
    temperature: float,
    max_tokens: int,
):
    try:
        stream_resp = await client.chat.completions.create(
            model=model_name,
            messages=messages,
            temperature=temperature,
            stream=True,
            max_tokens=max_tokens,
            extra_body={"options": {"num_predict": max_tokens}},
        )

        chunk_id = 0
        async for chunk in stream_resp:
            payload = ChatCompletionStreamPayload(stage_id=stage_id, chunk=chunk)
            yield serialize_sse_event(
                id=f"{stage_id}-chunk-{chunk_id}",
                event="chat_completion_chunk",
                data=payload
            )
            chunk_id += 1

        yield serialize_sse_event(
            id=str(chunk_id),
            event="done",
            data=DonePayload(stage_id=stage_id)
        )

    except asyncio.CancelledError:
        yield serialize_sse_event(
            id="0",
            event="cancel",
            data=CancelPayload(stage_id=stage_id)
        )
    except Exception as e:
        yield serialize_sse_event(
            id="0",
            event="error",
            data=ErrorPayload(stage_id=stage_id, error=str(e))
        )

async def run_chat_completion(
    stage_id: str,
    base_url: str,
    model_name: str,
    stream: bool,
    user_prompt: str,
    system_prompt: str,
    max_tokens: List[int],
    temperature: List[float],
) -> Union[JSONResponse, StreamingResponse]:
    """
    Chat-only completion. No tool logic. Streaming or regular response.
    Accepts explicit system/user prompt strings.
    """
    client = AsyncOpenAI(base_url=base_url, api_key="dummy")

    system_msg: ChatCompletionSystemMessageParam = {
        "role": "system",
        "content": system_prompt,
    }

    user_msg: ChatCompletionUserMessageParam = {
        "role": "user",
        "content": user_prompt,
    }

    messages: List[Union[ChatCompletionSystemMessageParam, ChatCompletionUserMessageParam]] = [
        system_msg,
        user_msg,
    ]
    max_tokens_val = max_tokens[0]
    temperature_val = temperature[0]

    if stream:
        return StreamingResponse(
            _stream_chat_response(
                stage_id,
                client,
                messages,
                model_name,
                temperature_val,
                max_tokens_val,
            ),
            media_type="text/event-stream",
        )

    try:
        resp = await client.chat.completions.create(
            model=model_name,
            messages=messages,
            temperature=temperature_val,
            stream=False,
            max_tokens=max_tokens_val,
            extra_body={"options": {"num_predict": max_tokens_val}},
        )

        return JSONResponse(
            content=TextStageOutput(
            stage_id=f"{stage_id}-chat-final",
            type="text",
            text=resp.choices[0].message.content
            ).model_dump(),
            media_type="application/json"
        )

    except Exception as e:
        return JSONResponse(
            content=TextStageOutput(
                stage_id=f"{stage_id}-chat-error",
                type="error",
                error=str(e)
            ).model_dump(),
            media_type="application/json"
        )
