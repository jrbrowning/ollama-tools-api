CONTEXT: FILE: fastapi_server/legacy/api/handlers/toolchain_runner.py
BRANCH: main

# LEARNING EXAMPLE ONLY - NOT FOR PRODUCTION

# pyright: reportGeneralTypeIssues=false
# pyright: reportMissingImports=false
# pyright: reportUndefinedVariable=false
# pyright: reportMissingTypeStubs=false
# pyright: reportUnknownMemberType=false
# pyright: reportUnknownVariableType=false
# pyright: reportUnknownArgumentType=false
# pyright: reportUnknownLambdaType=false
# pyright: reportUnknownParameterType=false
# pyright: reportOptionalMemberAccess=false
# pyright: reportOptionalSubscript=false
# pyright: reportOptionalCall=false
# pyright: reportOptionalIterable=false
# pyright: reportOptionalContextManager=false
# pyright: reportOptionalOperand=false
# pyright: reportGeneralTypeIssues=false
# pyright: reportMissingImports=false
import asyncio
import json
from typing import Any, Dict, List, Union

from fastapi.responses import JSONResponse, StreamingResponse
from models.events import (
    CancelPayload,
    ChatCompletionStreamPayload,
    DonePayload,
    ErrorPayload,
    ToolCompletionStreamPayload,
    serialize_sse_event,
)
from models.llm_response import CompletionErrorOutput, TextStageOutput, ToolStageOutput
from openai import AsyncOpenAI
from openai.types.chat import (
    ChatCompletionMessageToolCall,
    ChatCompletionSystemMessageParam,
    ChatCompletionUserMessageParam,
)
from openai.types.chat.chat_completion_message_tool_call import (
    Function as ToolCallFunction,
)
from toolkit.tools.get_weather_tool import GetWeatherTool

# from toolkit.tools.iss_tracker_tool import ISSTrackerTool
# from toolkit.tools.plant_care_tool import PlantCareAdvisorTool
from toolkit.tools.tool_types import ToolProtocol
from toolkit.utils.multi_tool_call_parts import MultiToolCallParts
from toolkit.utils.tool_registry import ToolRegistry
from toolkit.utils.tool_response_builder import build_tool_response_messages_multi


async def execute_tools_only(
    stage_id: str,
    client: AsyncOpenAI,
    registry: ToolRegistry,
    system_msg: ChatCompletionSystemMessageParam,
    user_msg: ChatCompletionUserMessageParam,
    model_name: str,
    max_tokens: int,
    temperature: float = 0.0,
) -> Dict[str, Any]:
    """Execute tools without AI synthesis. Returns tool results and validation."""

    resp = await client.chat.completions.create(
        model=model_name,
        messages=[system_msg, user_msg],
        tools=registry.all_specs(),
        tool_choice="auto",
        temperature=temperature,
        stream=False,
        max_tokens=max_tokens,
        extra_body={"options": {"num_predict": max_tokens}},
    )

    # OpenAI-style tool calls
    tool_calls = resp.choices[0].message.tool_calls or []

    # Qwen-style (JSON stream) fallback
    if not tool_calls:
        content: str = getattr(resp.choices[0].message, "content", "")
        try:
            parsed: dict[str, Any] = json.loads(content)
            tool_calls = [
                ChatCompletionMessageToolCall(
                    id="tool_0",
                    type="function",
                    function=ToolCallFunction(
                        name=parsed["name"],
                        arguments=json.dumps(parsed["arguments"]),
                    ),
                )
            ]
        except json.JSONDecodeError:
            pass

    if not tool_calls:
        return CompletionErrorOutput(
            stage_id=stage_id, type="error", message="One or more tool calls failed EXECUTION"
        ).model_dump()

    tool_call_map = MultiToolCallParts.from_completed(tool_calls)
    validation = registry.validate_all_tool_calls(tool_call_map)

    if not all(result["valid"] for result in validation.values()):
        return CompletionErrorOutput(
            stage_id=stage_id,
            type="error",
            message="One or more tool calls failed VALIDATION",
        ).model_dump()

    tool_results = registry.execute_all_tool_calls(tool_call_map)

    return ToolStageOutput(
        stage_id=stage_id,
        type="tool_results",
        tool_results=tool_results,
    ).model_dump()


async def execute_tools_with_synthesis_non_streaming(
    stage_id: str,
    client: AsyncOpenAI,
    registry: ToolRegistry,
    system_msg: ChatCompletionSystemMessageParam,
    user_msg: ChatCompletionUserMessageParam,
    model_name: str,
    max_tokens: List[int],
    temperature: List[float],
) -> Dict[str, Any]:
    """Execute tools and synthesize response (non-streaming)."""

    # Phase 1: Tool execution
    resp = await client.chat.completions.create(
        model=model_name,
        messages=[system_msg, user_msg],
        tools=registry.all_specs(),
        tool_choice="auto",
        temperature=temperature[0],
        stream=False,
        max_tokens=max_tokens[0],
        extra_body={"options": {"num_predict": max_tokens[0]}},
    )

    tool_calls = resp.choices[0].message.tool_calls or []

    # Qwen-style fallback
    if not tool_calls:
        content: str = getattr(resp.choices[0].message, "content", "")
        try:
            parsed: dict[str, Any] = json.loads(content)
            tool_calls = [
                ChatCompletionMessageToolCall(
                    id="tool_0",
                    type="function",
                    function=ToolCallFunction(
                        name=parsed["name"],
                        arguments=json.dumps(parsed["arguments"]),
                    ),
                )
            ]
        except json.JSONDecodeError:
            pass

    if not tool_calls:
        return CompletionErrorOutput(
            stage_id=stage_id, type="error", message="One or more tool calls failed EXECUTION"
        ).model_dump()

    tool_call_map = MultiToolCallParts.from_completed(tool_calls)
    validation = registry.validate_all_tool_calls(tool_call_map)

    if not all(result["valid"] for result in validation.values()):
        return CompletionErrorOutput(
            stage_id=stage_id, type="error", message="One or more tool calls failed EXECUTION"
        ).model_dump()

    tool_results = registry.execute_all_tool_calls(tool_call_map)

    # Phase 2: Synthesis
    followup_messages = build_tool_response_messages_multi(
        system_msg, user_msg, tool_calls, tool_results
    )

    second_resp = await client.chat.completions.create(
        model=model_name,
        messages=followup_messages,
        tools=registry.all_specs(),
        tool_choice="auto",
        temperature=temperature[1],
        stream=False,
        max_tokens=max_tokens[1],
        extra_body={"options": {"num_predict": max_tokens[1]}},
    )

    return TextStageOutput(
        stage_id=stage_id,
        type="text",
        text=second_resp.choices[0].message.content,
    ).model_dump()


async def _execute_tool_phase_streaming(
    stage_id: str,
    client: AsyncOpenAI,
    registry: ToolRegistry,
    system_msg: ChatCompletionSystemMessageParam,
    user_msg: ChatCompletionUserMessageParam,
    model_name: str,
    max_tokens: List[int],
    temperature: List[float],
    synthesis: bool | None = False,
):
    """Shared streaming logic for tool execution phase. Optionally continues to synthesis."""

    try:
        # Phase 1: Tool execution (streaming to collect tool calls)
        multi_tool_call_parts = MultiToolCallParts()

        stream_resp = await client.chat.completions.create(
            model=model_name,
            messages=[system_msg, user_msg],
            tools=registry.all_specs(),
            tool_choice="auto",
            temperature=temperature[0],  # Use tool temperature
            stream=True,
            max_tokens=max_tokens[0],  # Use tool max_tokens
            extra_body={"options": {"num_predict": max_tokens[0]}},
        )

        # Collect tool calls from stream
        chunk_id = 0
        async for chunk in stream_resp:
            multi_tool_call_parts.add_chunk(chunk)
            payload = ChatCompletionStreamPayload(stage_id=stage_id, chunk=chunk)
            yield serialize_sse_event(
                id=f"{stage_id}-chunk-{chunk_id}", event="tool_completion_chunk", data=payload
            )
            chunk_id += 1

        tool_calls = multi_tool_call_parts.to_message_tool_calls()
        tool_call_map = multi_tool_call_parts.to_message_tool_call_map()

        async for chunk in stream_resp:
            multi_tool_call_parts.add_chunk(chunk)
            payload = ChatCompletionStreamPayload(stage_id=stage_id, chunk=chunk)
            yield serialize_sse_event(
                id=f"{stage_id}-chunk-{chunk_id}", event="tool_completion_chunk", data=payload
            )
            chunk_id += 1

        tool_results = registry.execute_all_tool_calls(tool_call_map)
        payload = ToolCompletionStreamPayload(stage_id=stage_id, tool_results=tool_results)
        yield serialize_sse_event(id="0", event="tool_summary", data=payload)

        if not synthesis:
            # Tools only - we're done.   Better to check that complete from stream "done"
            yield serialize_sse_event(
                id=str(chunk_id), event="done", data=DonePayload(stage_id=stage_id)
            )
            return

        # Phase 2: AI synthesis (streaming)
        followup_messages = build_tool_response_messages_multi(
            system_msg, user_msg, tool_calls, tool_results
        )

        second_stream = await client.chat.completions.create(
            model=model_name,
            messages=followup_messages,
            temperature=temperature[1],  # Use synthesis temperature
            stream=True,
            max_tokens=max_tokens[1],  # Use synthesis max_tokens
            extra_body={"options": {"num_predict": max_tokens[1]}},
        )

        chunk_id = 0
        async for chunk in second_stream:
            delta = chunk.choices[0].delta
            if delta.content:
                payload = ChatCompletionStreamPayload(stage_id=stage_id, chunk=chunk)
                yield serialize_sse_event(
                    id=f"{stage_id}-chunk-integ-{chunk_id}",
                    event="tool_completion_chunk",
                    data=payload,
                )
                chunk_id += 1

        yield serialize_sse_event(
            id=str(chunk_id), event="done", data=DonePayload(stage_id=stage_id)
        )

    except asyncio.CancelledError:
        yield serialize_sse_event(id="0", event="cancel", data=CancelPayload(stage_id=stage_id))
    except Exception as e:
        yield serialize_sse_event(
            id="0", event="error", data=ErrorPayload(stage_id=stage_id, error=str(e))
        )


async def run_toolchain_logic(
    stage_id: str,
    base_url: str,
    model_name: str,
    stream: bool,
    max_tokens: List[int],
    temperature: List[float],
    user_prompt: str,
    system_prompt: str,
    synthesis: bool = True,
) -> Union[JSONResponse, StreamingResponse]:
    """
    Main entry point for tool execution.

    Args:
        synthesis: If True, executes tools + AI synthesis. If False, tools only.
        stream: If True, uses streaming for the operation.
    """

    client = AsyncOpenAI(base_url=base_url, api_key="dummy")
    tool_instances: List[ToolProtocol] = [GetWeatherTool()]
    registry = ToolRegistry(tool_instances)

    system_msg: ChatCompletionSystemMessageParam = {
        "role": "system",
        "content": system_prompt + " " + registry.concat_tool_system_prompt(),
    }

    user_msg: ChatCompletionUserMessageParam = {
        "role": "user",
        "content": user_prompt,
    }

    try:
        if not synthesis:
            # Tools only
            if stream:
                return StreamingResponse(
                    _execute_tool_phase_streaming(
                        stage_id,
                        client,
                        registry,
                        system_msg,
                        user_msg,
                        model_name,
                        max_tokens,
                        temperature,
                        synthesis,
                    ),
                    media_type="text/event-stream",
                )
            else:
                return JSONResponse(
                    await execute_tools_only(
                        stage_id,
                        client,
                        registry,
                        system_msg,
                        user_msg,
                        model_name,
                        max_tokens[0],
                        temperature[0],
                    ),
                    media_type="application/json",
                )

        else:
            # Tools + synthesis
            if stream:
                return StreamingResponse(
                    _execute_tool_phase_streaming(
                        stage_id,
                        client,
                        registry,
                        system_msg,
                        user_msg,
                        model_name,
                        max_tokens,
                        temperature,
                        synthesis,
                    ),
                    media_type="text/event-stream",
                )
            else:
                return JSONResponse(
                    await execute_tools_with_synthesis_non_streaming(
                        stage_id,
                        client,
                        registry,
                        system_msg,
                        user_msg,
                        model_name,
                        max_tokens,
                        temperature,
                    ),
                    media_type="application/json",
                )

    except Exception as e:
        return JSONResponse(
            CompletionErrorOutput(stage_id=stage_id, type="error", message=str(e)).model_dump(),
            media_type="application/json",
        )
