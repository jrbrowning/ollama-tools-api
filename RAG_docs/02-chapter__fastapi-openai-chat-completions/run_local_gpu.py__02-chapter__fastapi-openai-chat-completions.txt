CONTEXT: FILE: run_local_gpu.py
BRANCH: 02-chapter__fastapi-openai-chat-completions

# File: run_local_gpu.py
#!/usr/bin/env python3
import os
import shutil
import subprocess
import sys
import time

from pydantic import BaseModel

CONFIG_PATH = ".env"
PID_FILE = "local_gpu/ollama.pid"
LOG_FILE = "local_gpu/ollama.log"
SYSTEM_PROMPT_PATH = "lib/modelfiles/system.txt"


class Config(BaseModel):
    LOCAL_GPU: str
    ENABLE_LOCAL_GPU_MODEL: bool


def log(message: str):
    print(f"[{time.strftime('%Y-%m-%d %H:%M:%S')}] {message}")


def load_config() -> Config:
    if not os.path.isfile(CONFIG_PATH):
        log(f"‚ùå Missing config at {CONFIG_PATH}")
        sys.exit(1)

    config_data: dict[str, str] = {}
    with open(CONFIG_PATH) as f:
        for line in f:
            if "=" in line:
                k, v = line.strip().split("=", 1)
                config_data[k] = v.lower() if v.lower() in ["true", "false"] else v

    try:
        return Config(
            LOCAL_GPU=config_data["LOCAL_GPU"],
            ENABLE_LOCAL_GPU_MODEL=config_data["ENABLE_LOCAL_GPU_MODEL"] == "true",
        )
    except KeyError as e:
        log(f"‚ùå Missing required config variable: {e}")
        sys.exit(1)


def check_ollama_installed():
    if shutil.which("ollama") is None:
        log("‚ùå Ollama is not installed.")
        log("‚û°Ô∏è  Install from https://ollama.com/download")
        sys.exit(1)


def _daemon_running() -> bool:
    """Quick probe to avoid spawning duplicate daemons."""
    try:
        import requests

        res = requests.get("http://localhost:11434", timeout=0.8)
        return "Ollama is running" in res.text
    except Exception:
        return False


def start_ollama():
    if _daemon_running():
        log("‚ÑπÔ∏è Ollama daemon already running on :11434")
        return

    log("üöÄ Starting Ollama daemon on :11434 ‚Ä¶")
    env = os.environ.copy()
    env["OLLAMA_HOST"] = "0.0.0.0"
    with open(LOG_FILE, "a") as logf:
        proc = subprocess.Popen(["ollama", "serve"], stdout=logf, stderr=logf, env=env)
        with open(PID_FILE, "w") as pidf:
            pidf.write(str(proc.pid))


def wait_for_ready(seconds: int = 10):
    log("‚è≥ Waiting for Ollama to answer ‚Ä¶")
    import requests

    for _ in range(seconds):
        try:
            res = requests.get("http://localhost:11434", timeout=1)
            if "Ollama is running" in res.text:
                log("‚úÖ Ollama ready.")
                return
        except Exception:
            pass
        time.sleep(1)
    log("‚ùå Ollama did not start in time.")
    sys.exit(1)


def pull_or_verify_model(model: str, online: bool):
    if online:
        log(f"üì• Pulling model: {model}")
        try:
            subprocess.check_call(["ollama", "pull", model])
            log("‚úÖ Model pulled.")
        except subprocess.CalledProcessError:
            log(f"‚ùå Failed to pull model: {model}")
            sys.exit(1)
    else:
        log(f"üì¥ OFFLINE MODE: Verifying {model} is cached ‚Ä¶")
        output = subprocess.check_output(["ollama", "list"], text=True)
        if model not in output:
            log(f"‚ùå Model {model} not found in offline mode.")
            sys.exit(1)
        log(f"‚úÖ Model {model} is cached.")


def ensure_ollama_running():
    check_ollama_installed()
    start_ollama()
    wait_for_ready()


def build_modelfile(model: str) -> str:
    modelfile_path = f"/tmp/{model.replace(':', '_')}_Modelfile"
    with open(modelfile_path, "w") as mf:
        mf.write(f"FROM {model.split(':')[0]}\n\n")
        with open(SYSTEM_PROMPT_PATH) as sp:
            mf.write(sp.read())
    return modelfile_path


def up():
    config = load_config()
    ensure_ollama_running()
    model = config.LOCAL_GPU
    output = subprocess.check_output(["ollama", "list"], text=True)

    if model in output:
        log(f"‚úÖ Model '{model}' already exists ‚Äî ready.")
    else:
        log(f"üì° Model '{model}' not found.")
        if os.path.exists(SYSTEM_PROMPT_PATH):
            log("üß± Building with system prompt ‚Ä¶")
            modelfile = build_modelfile(model)
            subprocess.check_call(["ollama", "create", model, "-f", modelfile])
        else:
            pull_or_verify_model(model, config.ENABLE_LOCAL_GPU_MODEL)

    log(f"üèÅ Model '{model}' available at http://localhost:11434")


def rebuild():
    """
    Full reset: stop daemon ‚ûú remove model & base ‚ûú start fresh ‚ûú recreate/pull.
    """
    config = load_config()
    model = config.LOCAL_GPU
    base_model = model.split(":")[0]

    # 1. Stop daemon so layers are not locked
    stop()

    # 2. Remove all relevant tags/layers
    for target in {model, base_model}:
        try:
            subprocess.check_call(["ollama", "rm", target])
            log(f"üóëÔ∏è Removed: {target}")
        except subprocess.CalledProcessError:
            pass  # absent is fine

    # 3. Start daemon again
    ensure_ollama_running()

    # 4. Recreate or pull
    if os.path.exists(SYSTEM_PROMPT_PATH):
        log("üß± Rebuilding with system prompt ‚Ä¶")
        modelfile = build_modelfile(model)
        subprocess.check_call(["ollama", "create", model, "-f", modelfile])
    else:
        pull_or_verify_model(model, config.ENABLE_LOCAL_GPU_MODEL)

    log(f"üèÅ Rebuild complete ‚Äî '{model}' ready at http://localhost:11434")


def stop():
    if os.path.isfile(PID_FILE):
        log("üõë Stopping Ollama daemon ‚Ä¶")
        try:
            with open(PID_FILE) as pidf:
                pid = int(pidf.read().strip())
            os.kill(pid, 15)
        except Exception:
            log("‚ö†Ô∏è Could not kill Ollama process (may already be stopped).")
        os.remove(PID_FILE)
        log("‚úÖ Ollama process stopped.")
    else:
        log("‚ÑπÔ∏è No PID file; daemon not running.")


def print_help():
    print(
        """
run_local_gpu.py ‚Äî manage a local Ollama GPU instance

Usage:
  python run_local_gpu.py up        # start or reuse model
  python run_local_gpu.py rebuild   # force fresh build
  python run_local_gpu.py stop      # stop daemon
  python run_local_gpu.py list      # list local models
  python run_local_gpu.py help      # this help

Config (.env):
  LOCAL_GPU=<model[:tag]>           # e.g. llama3:8b
  ENABLE_LOCAL_GPU_MODEL=true|false

Ollama binds to :11434.
"""
    )


def list_models():
    check_ollama_installed()
    try:
        output = subprocess.check_output(["ollama", "list"], text=True)
        print("Available local models:\n")
        print(output)
    except subprocess.CalledProcessError:
        log("‚ùå Failed to list models.")
        sys.exit(1)


if __name__ == "__main__":
    cmd = sys.argv[1] if len(sys.argv) > 1 else "help"
    {
        "up": up,
        "rebuild": rebuild,
        "stop": stop,
        "list": list_models,
        "help": print_help,
    }.get(cmd, print_help)()
